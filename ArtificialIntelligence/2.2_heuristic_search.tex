% !TEX root = main.tex

\subsection{有信息搜索}
在无信息搜索中，我们从不估计边界集中最有期望(promising)获得最优解的结点，而是无区别地选择当前边界集中第一个结点。
然而事实上，针对不同问题我们是有对结点的先验知识(apriori knowledge)的，即从当前结点到目标结点的开销有多大。
而这就是有信息搜索(informed)，或者称为启发式搜索(heuristics)。

关键在于领域特定启发式函数$h(n)$的设计，它估计了\textbf{从结点$n$到目标结点的开销(cost)}。
注意满足目标状态的结点$h(n)=0$。

\subsubsection{贪心最优搜索(Greedy Best-First Search)}
直接使用$h(n)$对边界集进行排序，但这会导致贪心地选择\textbf{看上去}离目标结点开销最小的路径。

如果存在环路，贪心最优搜索是不完备的，会陷入死循环。
\begin{center}
\begin{tikzcd}
 & s\arrow[ld]\arrow[rd] &\\
n_1\arrow[bend right,d] & & n_3\arrow[d]\\
n_2\arrow[bend right,u] & & \text{Goal}
\end{tikzcd}
\end{center}

\subsubsection{A$^\star$搜索}
综合考虑当前已走的开销和未来估计的开销。
定义一个估值函数
\[f(n)=g(n)+h(n)\]
其中$g(n)$为路径到节点$n$的代价，$h(n)$为从$n$到目标节点的代价，采用$f(n)$对边界集内的节点进行排序。

$f(n)$需要满足下列两个性质。
\begin{definition}[可采纳的(admissibility)]
假设所有代价$c(n1\to n2)\geq\eps>0$，令$h^\star(n)$为从节点$n$到目标节点$\infty$的最优解代价\footnote{如果没有路径则$h^\star(n)=\infty$}，若
\[\forall n:\;h(n)\leq h^\star(n)\]
则称$h(n)$是可采纳的。
即一个可采纳的启发式函数总是\textemph{低估}了当前结点到目标结点的真实开销（这样才能保证最优解不被排除）。
因此对于任何目标结点$g$都有$h(g)=0$。
\end{definition}
\begin{definition}[一致性(consistency)/单调性(monotonicity)]
若对于所有的结点$n_1$和$n_2$，$h(n)$满足（三角不等式）
\[h(n_1)\leq c(n_1\to n_2)+h(n_2)\]
则称$h(n)$是单调的。
\end{definition}

\begin{theorem}
一致性蕴含可采纳性
\end{theorem}
\begin{analysis}
分类讨论
\begin{itemize}
	\item 当结点$n$没有到目标结点的路径，则$h(n)\leq h^\star(n)=\infty$恒成立
	\item 令$n=n_1\to n_2\to\cdots\to n_k$为从结点$n$到目标结点的最优路径，则可以用数学归纳法证明$\forall i:\;h(n_i)\leq h^\star(n_i)$，如下从后往前推
	\[h(n_{i})\leq c(n_i\to n_{i+1})+h(n_{i+1})\leq c(n_i\to n_{i+1}) + h^\star(n_{i+1})=h^\star(n_i)\]
\end{itemize}
\end{analysis}

\begin{theorem}
可采纳性蕴含最优性
\end{theorem}
\begin{analysis}
假设最优解有开销$C^\star$，则任何最优解一定会在开销大于$C^\star$的路径之前被展开（有限条路径）。

反证若路径$p$的代价$c(p)$大于最优解路径代价$c(p^\star)$且$p$在$p^\star$之前被扩展，则一定存在一个节点$n$在$p^\star$上且仍在边界集中，因此
\[c(p)\leq f(p)\leq f(n)=g(n)+h(n)\leq g(n)+h^\star(n)=c(p^\star)\]
矛盾。故在最优解展开之前的路径一定有开销$\leq C^\star$，最终我们一定会检测到最优解，而且次优解不会在最优解之前被检测。
\end{analysis}

做环检测可能导致找不到最优解。
可采纳性不能保证最优解，但\textemph{单调性环检测保证最优解}。

单调性有以下几个性质
\begin{proposition}
路径上的$f$一定是非递减的
\end{proposition}
\begin{analysis}
\[\begin{aligned}
f(n)&=g(n)+h(n)\\
&\leq g(n)+c(n\to n')+h(n')\\
&= g(n')+h(n')\\
&= f(n')
\end{aligned}\]
\end{analysis}
\begin{proposition}
如果$n_2$在$n_1$之后被扩展，则$f(n_1)\leq f(n_2)$
\end{proposition}
\begin{proposition}
当$n$在任何小于$f$值得路径之前被展开
\end{proposition}
\begin{proposition}
$A^\star$算法第一次展开某个状态时，它已经找到了到达那个状态的最小开销路径。
\end{proposition}

\subsubsection{迭代加深A$^\star$(IDA)算法}
$A^\star$算法有和BFS或UCS同样的空间复杂性问题，而迭代加深$A^\star$算法同样解决空间复杂度的问题。
与迭代加深类似，但IDA$^\star$则用$f=g+h$作为截断阈值。
每一轮迭代中选择上一轮中超过截断阈值最小的$f$。

\subsubsection{构造启发式函数}
常常需要考虑一个更加简单的问题（松弛问题），然后让$h(n)$为到达一个简单问题解的开销。
比如考虑$A$和$B$之间没有屏障，$A$和$B$相邻等。

\begin{example}
现有积木若干，积木可以放在桌子上，也可以放在另一块积木上面。
有两种操作：
\begin{enumerate}
	\item \verb'move(x,y)'：把积木x放到积木y上面，前提是积木x和积木y上面都没有其他积木
	\item \verb'moveToTable(x)'：把积木x放到桌子上，前提是积木x上面无其他积木，且积木x不在桌子上
\end{enumerate}
设计一个可采纳的启发式函数$h(n)$
\end{example}
\begin{analysis}
$h(n)=h_1(n)+h_2(n)$，其中$h_1(n)$为在目标状态积木的数目，$h_2(n)$为符合上下关系的积木数目（$A$在$B$下，且$A$在目标状态）
\end{analysis}
\begin{theorem}
在松弛问题中的最优解也是原问题中的一个可满足的启发式函数。
\end{theorem}
\begin{definition}[支配]
如果$h_2$支配(dominate)$h_1$且除了目标结点都可满足，则$h_1(n)\leq h_2(n)$
\end{definition}