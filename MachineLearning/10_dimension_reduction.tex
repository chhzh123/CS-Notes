% !TEX root = main.tex

\section{降维与度量学习}
线性降维方法：对原始高维空间进行线性变换。
给定$d$维空间中的样本$X=(\vx_1,\vx_2,\ldots,\vx_m)\in\rr^{d\times m}$，
变换后得到$d'\leq d$维空间中的样本$Z=W^\T X$，
其中$W\in\rr^{d\times d'}$是变换矩阵，$Z\in\rr^{d'\times m}$是样本在新空间中的表达。

\subsection{k近邻学习}
k近邻(k-nearest neighbor, kNN)是常用的监督学习方法。
给定测试样本，基于某种距离度量找出训练集中与\textbf{最为靠近}的$k$个训练样本，然后基于这$k$个邻居的信息进行预测。
在分类任务中可以采用\textbf{投票法}，在回归任务中可以采用\textbf{平均法}。

对于categorical属性，有序关系的可直接编码为数字，没有序关系的则要用独热码。
快速实现kNN算法可以用KD树，快速筛选出前$k$个距离最小的点。

kNN是懒惰学习(lazy learning)的著名代表，在训练阶段仅仅将样本保存起来，训练时间开销为0，待收到测试样本才进行处理。
\begin{figure}[H]
\centering
\includegraphics[width=0.4\linewidth]{fig/kNN.png}
\end{figure}

这里距离的选择包括欧氏距离、闵可夫斯基距离、曼哈顿距离等。
可以通过给每个特征/样本点加权来分配重要性。
同时为避免属性之间的尺度不均，可以先做归一化$(v_i-\min_j v_j)/(\max_j v_j-\min_j v_j)$，将特征值均映射到$[0,1]$的区间。

$k$太小则对噪声敏感，$k$太大则可能包含其他类别的很多点，经验法则取$k<\sqrt{n}$，$n$为样本数目。

\begin{itemize}
	\item 优点：直观易理解，可应用到不同分布的数据
	\item 缺点：分类时间长，$k$玄学选择，需要大规模样本
\end{itemize}

\subsection{主成分分析}
一般来说，想要获得低维子空间，最简单的方式是对原始高维空间进行线性变换。
给定$d$维空间中的样本$X=(\vx_1,\vx_2,\ldots,\vx_m)\in\rr^{d\times m}$变换之后得到$d'\leq d$维空间中的样本$Z=W^\T X$，其中$W\in\rr^{d\times d'}$为变换矩阵，$Z\in\rr^{d'\times m}$是样本在新空间中的表达。
基于线性变换来进行降维的方法称为线性降维方法，对低维子空间性质的不同要求可通过对$W$施加不同的约束来实现。

主成分分析(Principle Component Analysis, PCA)希望找到这样的超平面具有这样的性质：
\begin{itemize}
	\item 最近重构性：样本点到这个超平面的距离都足够近，对样本中心化$\sum_i\vx_i=\vzero$，再假定投影变换后得到的新坐标系为$\{\vw_1,\vw_2,\ldots,\vw_d\}$，其中$\vw_i$是标准正交基向量
	\[\norm{\vw_i}_2=1,\vw_i^\T\vw_j=0\;(i\ne j)\]
	\item 最大可分性：样本点在这个超平面上的投影尽可能分开
	\[\begin{aligned}
	\max_{W} &\qquad \optr(W^\T XX^\T W)\\
	\text{s.t.} &\qquad W^\T W=I
	\end{aligned}\]
\end{itemize}

算法流程如下：输入样本集$D=\{\vx_i\}_{i=1}^m$和低维空间维数$d'$
\begin{enumerate}
	\item 对所有样本进行中心化：$\vx_i\gets \vx_i-\frac{1}{m}\sum_{i=1}^m\vx_i$
	\item 计算样本的协方差矩阵$XX^\T$
	\item 对协方差矩阵$XX^\T$做特征值分解
	\item 取最大的$d'$个特征值所对应的特征向量$\vw_1,\vw_2,\ldots,\vw_{d'}$
	\item 输出投影矩阵$W=(\vw_1,\vw_2,\ldots,\vw_{d'})$
\end{enumerate}

降维虽然会导致信息损失，但一方面舍弃这些信息后能使样本的采样密度增大，另一方面当数据收到噪声影响时，最小的特征值所对应的特征向量往往与噪声有关，舍弃可以起到去噪效果。

\subsection{核化线性降维}
线性降维方法假设从高维空间到低维空间的函数映射是线性的，然而，在不少现实任务中，可能需要非线性映射才能找到恰当的低维嵌入。

非线性降维的一种常用方法，是基于核技巧对线性降维方法进行核化(kernelized)。

\subsection{流形学习}
局部线性嵌入(Locally Linear Embedding, LLE)试图保持邻域内的线性关系\footnote{与现在的GNN有异曲同工之妙}，并使得该线性关系在降维后的空间中继续保持。
\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{fig/LLE.png}
\end{figure}

\subsection{自编码器}
自编码器(autoencoder)是一种前馈神经网络，输入$\vx$，预测$\vx$。
最简单即单层的神经网络，$\tilde{\vx}=W_2W_1\vx$，计算平方误差
\[L(\vx,\tilde{\vx})=\norm{\vx-\tilde{\vx}}^2\]

将高维数据映射到二维来做可视化；以无监督方式学习抽象特征，进而可应用到有监督的任务中。